<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"  lang="en-us">
    <title>Looking into 19th century ads from a Luxembourguish newspaper with R - Econometrics and Free Software</title>
    <meta name="generator" content="Hugo 0.25.1" />

    
    <meta name="description" content="A blog about econometrics, free software, and R">
    
    <link rel="canonical" href="../../blog/2019-01-04-newspapers/">
    
    <meta name="author" content="Bruno Rodrigues">
    
    <meta name="generator" content="Hugo 0.25.1" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:url" content="/blog/2019-01-04-newspapers/">
    <meta property="og:title" content="Econometrics and Free Software">
    <meta property="og:image" content="/map[url:logo.png width:50 height:50 alt:Logo]">
    <meta name="apple-mobile-web-app-title" content="Econometrics and Free Software">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="shortcut icon" type="image/x-icon" href="../../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../../images/favicon.ico">

    
    <link rel="stylesheet" href="../../css/bootstrap.min.css" media="screen">
    <link rel="stylesheet" href="../../css/custom.css">
    <link rel="stylesheet" href="../../css/pygments.css">
    
    <link rel="stylesheet" href="../../css/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
    <link rel="stylesheet" href="../../css/helpers/jquery.fancybox-buttons.css?v=1.0.5" type="text/css" media="screen" />
  </head>
  <body>


<div class="row">
<div class="navbar navbar-default navbar-fixed-top">
<div class="container">
  <div class="navbar-header">
    <a href="../../" class="navbar-brand">Econometrics and Free Software</a>
    <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
  </div>
  <div class="navbar-collapse collapse" id="navbar-main">

    <ul class="nav navbar-nav navbar-right">
      <li><a href="https://www.buymeacoffee.com/brodriguesco">Donate</a></li>
      <li><a href="https://www.youtube.com/c/BrunoRodrigues1988">Youtube</a></li>
      <li><a href="https://github.com/b-rodrigues/">Github</a></li>
      <li><a href="https://twitter.com/brodriguesco/">Twitter</a></li>
    </ul>
  </div>
</div>

</div>

<div class="container">

  <div>
    <div class="row">
      <div class="col-lg-3 col-md-3 col-sm-4">
        

<div class="list-group table-of-contents">

  
    <a class="list-group-item" href=#><b>About Me</b></a>
    
    <div class="list-group">
      
      <a class="list-group-item" href="../../about/about/">Who am I?</a>
      
      <a class="list-group-item" href="../../about/projects/">Projects</a>
      
      <a class="list-group-item" href="../../about/research/">Research</a>
      
      <a class="list-group-item" href="../../about/software/">Software</a>
      
    </div>
    
  
    <a class="list-group-item" href=#><b>Blog</b></a>
    
    <div class="list-group">
      
      <a class="list-group-item" href="../../blog/2018-11-25-tidy_cv/">A tutorial on tidy cross-validation with R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-03-nethack_analysis/">Analyzing NetHack data, part 1: What kills the players</a>
      
      <a class="list-group-item" href="../../blog/2018-11-10-nethack_analysis_part2/">Analyzing NetHack data, part 2: What players kill the most</a>
      
      <a class="list-group-item" href="../../blog/2019-02-04-newspapers_shiny_app_tutorial/">Building a shiny app to explore historical newspapers: a step-by-step guide</a>
      
      <a class="list-group-item" href="../../blog/2019-03-03-historical_vowpal/">Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</a>
      
      <a class="list-group-item" href="../../blog/2019-03-05-historical_vowpal_part2/">Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</a>
      
      <a class="list-group-item" href="../../blog/2019-06-20-tidy_eval_saga/">Curly-Curly, the successor of Bang-Bang</a>
      
      <a class="list-group-item" href="../../blog/2018-07-08-rob_stderr/">Dealing with heteroskedasticity; regression with robust standard errors using R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-14-luxairport/">Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport</a>
      
      <a class="list-group-item" href="../../blog/2018-10-05-ggplot2_purrr_officer/">Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer</a>
      
      <a class="list-group-item" href="../../blog/2019-04-28-diffindiff_part1/">Fast food, causality and R packages, part 1</a>
      
      <a class="list-group-item" href="../../blog/2019-05-04-diffindiff_part2/">Fast food, causality and R packages, part 2</a>
      
      <a class="list-group-item" href="../../blog/2019-05-18-xml2/">For posterity: install {xml2} on GNU/Linux distros</a>
      
      <a class="list-group-item" href="../../blog/2018-06-24-fun_ts/">Forecasting my weight with R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-01-nethack/">From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack</a>
      
      <a class="list-group-item" href="../../blog/2019-03-31-tesseract/">Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</a>
      
      <a class="list-group-item" href="../../blog/2018-06-10-scraping_pdfs/">Getting data from pdfs using the pdftools package</a>
      
      <a class="list-group-item" href="../../blog/2018-10-21-lux_elections/">Getting the data from the Luxembourguish elections out of Excel</a>
      
      <a class="list-group-item" href="../../blog/2018-09-11-human_to_machine/">Going from a human readable Excel file to a machine-readable csv with {tidyxl}</a>
      
      <a class="list-group-item" href="../../blog/2019-04-07-historical_newspaper_scraping_tesseract/">Historical newspaper scraping with {tesseract} and R</a>
      
      <a class="list-group-item" href="../../blog/2018-09-15-time_use/">How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data</a>
      
      <a class="list-group-item" href="../../blog/2018-04-14-playing_with_furrr/">Imputing missing values in parallel using {furrr}</a>
      
      <a class="list-group-item" href="../../blog/2019-06-12-intermittent/">Intermittent demand, Croston and Die Hard</a>
      
      <a class="list-group-item" href="../../blog/2019-01-04-newspapers/">Looking into 19th century ads from a Luxembourguish newspaper with R</a>
      
      <a class="list-group-item" href="../../blog/2019-01-13-newspapers_mets_alto/">Making sense of the METS and ALTO XML standards</a>
      
      <a class="list-group-item" href="../../blog/2018-12-15-lubridate_africa/">Manipulate dates easily with {lubridate}</a>
      
      <a class="list-group-item" href="../../blog/2019-02-10-stringr_package/">Manipulating strings with the {stringr} package</a>
      
      <a class="list-group-item" href="../../blog/2018-10-27-lux_elections_analysis/">Maps with pie charts on top of each administrative division: an example with Luxembourg&#39;s elections data</a>
      
      <a class="list-group-item" href="../../blog/2018-07-01-tidy_ive/">Missing data imputation and instrumental variables regression: the tidy approach</a>
      
      <a class="list-group-item" href="../../blog/2019-08-17-modern_r/">Modern R with the tidyverse is available on Leanpub</a>
      
      <a class="list-group-item" href="../../blog/2018-12-24-modern_objects/">Objects types and some useful R functions for beginners</a>
      
      <a class="list-group-item" href="../../blog/2019-03-20-pivot/">Pivoting data frames just got easier thanks to `pivot_wide()` and `pivot_long()`</a>
      
      <a class="list-group-item" href="../../blog/2018-12-30-reticulate/">R or Python? Why not both? Using Anaconda Python within R with {reticulate}</a>
      
      <a class="list-group-item" href="../../blog/2018-11-15-tidy_gridsearch/">Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach</a>
      
      <a class="list-group-item" href="../../blog/2018-12-27-fun_gganimate/">Some fun with {gganimate}</a>
      
      <a class="list-group-item" href="../../blog/2019-10-05-parallel_maxlik/">Split-apply-combine for Maximum Likelihood Estimation of a linear model</a>
      
      <a class="list-group-item" href="../../blog/2019-07-19-statmatch/">Statistical matching, or when one single data source is not enough</a>
      
      <a class="list-group-item" href="../../blog/2018-11-21-lux_castle/">The best way to visit Luxembourguish castles is doing data science &#43; combinatorial optimization</a>
      
      <a class="list-group-item" href="../../blog/2019-05-19-spacemacs/">The never-ending editor war (?)</a>
      
      <a class="list-group-item" href="../../blog/2018-09-08-steam_linux/">The year of the GNU&#43;Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse</a>
      
      <a class="list-group-item" href="../../blog/2019-01-31-newspapers_shiny_app/">Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</a>
      
      <a class="list-group-item" href="../../blog/2018-11-16-rgenoud_arima/">Using a genetic algorithm for the hyperparameter optimization of a SARIMA model</a>
      
      <a class="list-group-item" href="../../blog/2019-06-04-cosine_sim/">Using cosine similarity to find matching documents: a tutorial using Seneca&#39;s letters to his friend Lucilius</a>
      
      <a class="list-group-item" href="../../blog/2019-08-14-lpm/">Using linear models with binary dependent variables, a simulation study</a>
      
      <a class="list-group-item" href="../../blog/2018-12-21-tidyverse_pi/">Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods</a>
      
      <a class="list-group-item" href="../../blog/2018-12-02-hyper-parameters/">What hyper-parameters are, and what to do with them; an illustration with ridge regression</a>
      
      <a class="list-group-item" href="../../blog/2019-09-03-disk_frame/">{disk.frame} is epic</a>
      
      <a class="list-group-item" href="../../blog/2018-04-15-announcing_pmice/">{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}</a>
      
      <a class="list-group-item" href="../../blog/2017-12-27-build_formulae/">Building formulae</a>
      
      <a class="list-group-item" href="../../blog/2017-11-14-peace_r/">Functional peace of mind</a>
      
      <a class="list-group-item" href="../../blog/2018-04-10-brotools_describe/">Get basic summary statistics for all the variables in a data frame</a>
      
      <a class="list-group-item" href="../../blog/2018-03-03-sparklyr_h2o_rsparkling/">Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash</a>
      
      <a class="list-group-item" href="../../blog/2018-02-16-importing_30gb_of_data/">Importing 30GB of data into R with sparklyr</a>
      
      <a class="list-group-item" href="../../blog/2017-03-27-introducing_brotools/">Introducing brotools</a>
      
      <a class="list-group-item" href="../../blog/2018-01-03-lists_all_the_way/">It&#39;s lists all the way down</a>
      
      <a class="list-group-item" href="../../blog/2018-01-05-lists_all_the_way2/">It&#39;s lists all the way down, part 2: We need to go deeper</a>
      
      <a class="list-group-item" href="../../blog/2018-03-12-keep_trying/">Keep trying that api call with purrr::possibly()</a>
      
      <a class="list-group-item" href="../../blog/2017-06-19-dplyr-0-70-tutorial/">Lesser known dplyr 0.7* tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-02-17-lesser_known_tricks/">Lesser known dplyr tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-03-24-lesser_known_purrr/">Lesser known purrr tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>
      
      <a class="list-group-item" href="../../blog/2018-01-19-mapping_functions_with_any_cols/">Mapping a list of functions to a list of datasets with a list of columns as arguments</a>
      
      <a class="list-group-item" href="../../blog/2018-02-11-census-random_forest/">Predicting job search by training a random forest on an unbalanced dataset</a>
      
      <a class="list-group-item" href="../../blog/2017-12-17-teaching_tidyverse/">Teaching the tidyverse to beginners</a>
      
      <a class="list-group-item" href="../../blog/2017-08-27-why_tidyeval/">Why I find tidyeval useful</a>
      
      <a class="list-group-item" href="../../blog/2017-07-27-spread_rename_at/">tidyr::spread() and dplyr::rename_at() in action</a>
      
      <a class="list-group-item" href="../../blog/2017-10-26-margins_r/">Easy peasy STATA-like marginal effects with R</a>
      
      <a class="list-group-item" href="../../blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/">Functional programming and unit testing for data munging with R available on Leanpub</a>
      
      <a class="list-group-item" href="../../blog/2017-02-17-how_to_use_jailbreakr/">How to use jailbreakr</a>
      
      <a class="list-group-item" href="../../blog/2017-01-07-my-free-book-has-a-cover/">My free book has a cover!</a>
      
      <a class="list-group-item" href="../../blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/">Work on lists of datasets instead of individual datasets by using functional programming</a>
      
      <a class="list-group-item" href="../../blog/2013-01-29-method-of-simulated-moments-with-r/">Method of Simulated Moments with R</a>
      
      <a class="list-group-item" href="../../blog/2012-12-11-new-website/">New website!</a>
      
      <a class="list-group-item" href="../../blog/2013-11-07-gmm-with-rmd/">Nonlinear Gmm with R - Example with a logistic regression</a>
      
      <a class="list-group-item" href="../../blog/2013-01-16-simulated-maximum-likelihood-with-r/">Simulated Maximum Likelihood with R</a>
      
      <a class="list-group-item" href="../../blog/2015-11-11-bootstrapping-did-with-r/">Bootstrapping standard errors for difference-in-differences estimation with R</a>
      
      <a class="list-group-item" href="../../blog/2016-06-21-careful-with-trycatch/">Careful with tryCatch</a>
      
      <a class="list-group-item" href="../../blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/">Data frame columns as arguments to dplyr functions</a>
      
      <a class="list-group-item" href="../../blog/2015-02-22-export-r-output-to-file/">Export R output to a file</a>
      
      <a class="list-group-item" href="../../blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/">I&#39;ve started writing a &#39;book&#39;: Functional programming and unit testing for data munging with R</a>
      
      <a class="list-group-item" href="../../blog/2015-01-12-introduction-to-programming-econometrics-with-r/">Introduction to programming econometrics with R</a>
      
      <a class="list-group-item" href="../../blog/2016-07-30-merge-a-list-of-datasets-together/">Merge a list of datasets together</a>
      
      <a class="list-group-item" href="../../blog/2014-04-23-r-s4-rootfinding/">Object Oriented Programming with R: An example with a Cournot duopoly</a>
      
      <a class="list-group-item" href="../../blog/2014-11-11-benchmarks-r-blas-atlas-rro/">R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?</a>
      
      <a class="list-group-item" href="../../blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/">Read a lot of datasets at once with R</a>
      
      <a class="list-group-item" href="../../blog/2016-03-31-unit-testing-with-r/">Unit testing with R</a>
      
      <a class="list-group-item" href="../../blog/2015-05-03-update-introduction-r-programming/">Update to Introduction to programming econometrics with R</a>
      
      <a class="list-group-item" href="../../blog/2013-12-31-r-cas/">Using R as a Computer Algebra System with Ryacas</a>
      
    </div>
    
  

</div>

      </div>
      <div class="col-lg-9 col-md-9 col-sm-8">
        <div>
           <h1 id="main">Looking into 19th century ads from a Luxembourguish newspaper with R</h1>
           <span class="article-date">January 4, 2019</span>
        </div>
        <div style="text-align:center;">
<p><a href="https://www.youtube.com/watch?v=0xzN6FM5x_E">
<img src="../../img/Wales.jpg" title = "Sometimes ads are better than this. Especially if it's Flex Tape ® ads."></a></p>
</div>
<p>The <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a> published
some very interesting data sets; scans of historical newspapers! There are several data sets that
you can download, from 250mb up to 257gb. I decided to take a look at the 32gb “ML Starter Pack”.
It contains high quality scans of one year of the <em>L’indépendence Luxembourgeoise</em> (Luxembourguish
independence) from the year 1877. To make life easier to data scientists, the national library
also included ALTO and METS files (which is a XML schema that is used to describe the layout and
contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed
by R.</p>
<p><em>L’indépendence Luxembourgeoise</em> is quite interesting in that it is a Luxembourguish newspaper written
in French. Luxembourg always had 3 languages that were used in different situations, French, German
and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life
and to speak to their baker.
Historically however, it was not used for the press or in politics. Instead it was German that
was used for the press (or so I thought) and French in politics (only in
<a href="http://legilux.public.lu/eli/etat/leg/loi/1984/02/24/n1/jo">1984</a> was Luxembourguish made
an official Language of Luxembourg).
It turns out however that <em>L’indépendence Luxembourgeoise</em>, a daily newspaper that does not exist
anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons:
I first started with the <em>Luxemburger Wort</em> (Luxembourg’s Word I guess would be a translation), which
still exists today, but which is in German. And at that time, German was written using the Fraktur
font, which makes it barely readable. Look at the alphabet in Fraktur:</p>
<pre><code>𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅
𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟</code></pre>
<p>It’s not like German is already hard enough, they had to invent the least readable font ever to write
German in, to make extra sure it would be hell to decipher.</p>
<p>So basically I couldn’t be bothered to try to read a German newspaper in Fraktur. That’s when I noticed
the <em>L’indépendence Luxembourgeoise</em>… A Luxembourguish newspaper? Written in French? Sounds
interesting.</p>
<p>And oh boy. Interesting it was.</p>
<p>19th century newspapers articles were something else. There’s this article for instance:</p>
<p><img src="../../img/pray%20for%20senators.png" /><!-- --></p>
<p>For those of you that do not read French, this article relates that in France, the ministry of
justice required priests to include prayers on the Sunday that follows the start of the new season
of parliamentary discussions, in order for God to provide senators his help.</p>
<p>There this gem too:</p>
<p><img src="../../img/tallest_soldier.jpg" /><!-- --></p>
<p>This article presents the tallest soldier of the German army, called Emhke, and nominated by the
German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall
and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).</p>
<p>Anyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained
ads and other announcements. For example, there’s this ad for a pharmacy:</p>
<p><img src="../../img/pharmacy.png" /><!-- --></p>
<p>that sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:</p>
<p><img src="../../img/upside_down.png" /><!-- --></p>
<p>which is literally upside down in the newspaper (the one from the 10th of April 1877). I don’t
know if it’s a mistake or if it’s a marketing ploy, but it did catch my attention, 140 years later,
so <em>bravo</em>. This is an announcement made by a shop owner that wants to sell all his merchandise
for cheap, perhaps to make space for new stuff coming in?</p>
<p>So I decided brush up on my natural language processing skills with R and do topic modeling on these ads.
The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads.
So it will probably be difficult to clearly isolate topics. But let’s try nonetheless.
First of all, let’s load all the <code>.xml</code> files that contain the data. These files look like this:</p>
<pre><code>&lt;TextLine ID=&quot;LINE6&quot; STYLEREFS=&quot;TS11&quot; HEIGHT=&quot;42&quot; WIDTH=&quot;449&quot; HPOS=&quot;165&quot; VPOS=&quot;493&quot;&gt;
                                    &lt;String ID=&quot;S16&quot; CONTENT=&quot;l’après-midi,&quot; WC=&quot;0.638&quot; CC=&quot;0803367024653&quot; HEIGHT=&quot;42&quot; WIDTH=&quot;208&quot; HPOS=&quot;165&quot; VPOS=&quot;493&quot;/&gt;
                                    &lt;SP ID=&quot;SP11&quot; WIDTH=&quot;24&quot; HPOS=&quot;373&quot; VPOS=&quot;493&quot;/&gt;
                                    &lt;String ID=&quot;S17&quot; CONTENT=&quot;le&quot; WC=&quot;0.8&quot; CC=&quot;40&quot; HEIGHT=&quot;30&quot; WIDTH=&quot;29&quot; HPOS=&quot;397&quot; VPOS=&quot;497&quot;/&gt;
                                    &lt;SP ID=&quot;SP12&quot; WIDTH=&quot;14&quot; HPOS=&quot;426&quot; VPOS=&quot;497&quot;/&gt;
                                    &lt;String ID=&quot;S18&quot; CONTENT=&quot;Gouverne&quot; WC=&quot;0.638&quot; CC=&quot;72370460&quot; HEIGHT=&quot;31&quot; WIDTH=&quot;161&quot; HPOS=&quot;440&quot; VPOS=&quot;496&quot; SUBS_TYPE=&quot;HypPart1&quot; SUBS_CONTENT=&quot;Gouvernement&quot;/&gt;
                                    &lt;HYP CONTENT=&quot;-&quot; WIDTH=&quot;11&quot; HPOS=&quot;603&quot; VPOS=&quot;514&quot;/&gt;
                                  &lt;/TextLine&gt;
                        &lt;TextLine ID=&quot;LINE7&quot; STYLEREFS=&quot;TS11&quot; HEIGHT=&quot;41&quot; WIDTH=&quot;449&quot; HPOS=&quot;166&quot; VPOS=&quot;541&quot;&gt;
                                    &lt;String ID=&quot;S19&quot; CONTENT=&quot;ment&quot; WC=&quot;0.725&quot; CC=&quot;0074&quot; HEIGHT=&quot;26&quot; WIDTH=&quot;81&quot; HPOS=&quot;166&quot; VPOS=&quot;545&quot; SUBS_TYPE=&quot;HypPart2&quot; SUBS_CONTENT=&quot;Gouvernement&quot;/&gt;
                                    &lt;SP ID=&quot;SP13&quot; WIDTH=&quot;24&quot; HPOS=&quot;247&quot; VPOS=&quot;545&quot;/&gt;
                                    &lt;String ID=&quot;S20&quot; CONTENT=&quot;Royal&quot; WC=&quot;0.62&quot; CC=&quot;74503&quot; HEIGHT=&quot;41&quot; WIDTH=&quot;100&quot; HPOS=&quot;271&quot; VPOS=&quot;541&quot;/&gt;
                                    &lt;SP ID=&quot;SP14&quot; WIDTH=&quot;26&quot; HPOS=&quot;371&quot; VPOS=&quot;541&quot;/&gt;
                                    &lt;String ID=&quot;S21&quot; CONTENT=&quot;Grand-Ducal&quot; WC=&quot;0.682&quot; CC=&quot;75260334005&quot; HEIGHT=&quot;32&quot; WIDTH=&quot;218&quot; HPOS=&quot;397&quot; VPOS=&quot;541&quot;/&gt;
                                  &lt;/TextLine&gt;</code></pre>
<p>I’m interested in the “CONTENT” tag, which contains the words. Let’s first get that into R.</p>
<p>Load the packages, and the files:</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(topicmodels)
library(brotools)

ad_pages &lt;- str_match(list.files(path = &quot;./&quot;, all.files = TRUE, recursive = TRUE), &quot;.*4-alto.xml&quot;) %&gt;%
    discard(is.na)</code></pre>
<p>I save the path of all the pages at once into the <code>ad_pages</code> variables. To understand how and why
this works, you must take a look at the hierarchy of the folder:</p>
<p><img src="../../img/layout.png" /><!-- --></p>
<p>Inside each of these folder, there is a <code>text</code> folder, and inside this folder there are the <code>.xml</code>
files. Because this structure is bit complex, I use the <code>list.files()</code> function with the
<code>all.files</code> and <code>recursive</code> argument set to <code>TRUE</code> which allow me to dig deep into the folder
structure and list every single file. I am only interested into the 4th page though, so that’s why
I use <code>str_match()</code> to only keep the 4th page using the <code>".*4-alto.xml"</code> regular expression. This
is the right regular expression, because the files are named like so:</p>
<pre><code>1877-12-29_01-00004-alto.xml</code></pre>
<p>So in the end, <code>ad_pages</code> is a list of all the paths to these files. I then write a function
to extract the contents of the “CONTENT” tag. Here is the function.</p>
<pre class="r"><code>get_words &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    page_name &lt;- str_extract(page_path, &quot;1.*(?=-0000)&quot;) 
    
    page %&gt;%  
        str_split(&quot;\n&quot;, simplify = TRUE) %&gt;% 
        keep(str_detect(., &quot;CONTENT&quot;)) %&gt;% 
        str_extract(&quot;(?&lt;=CONTENT)(.*?)(?=WC)&quot;) %&gt;% 
        discard(is.na) %&gt;% 
        str_extract(&quot;[:alpha:]+&quot;) %&gt;% 
        tolower %&gt;% 
        as_tibble %&gt;% 
        rename(tokens = value) %&gt;% 
        mutate(page = page_name)
}</code></pre>
<p>This function takes the path to a page as argument, and returns a tibble with the two columns: one
containing the words, which I called <code>tokens</code> and the second the name of the document this word
was found. I uploaded on <code>.xml</code> file
<a href="https://gist.github.com/b-rodrigues/a22d2aa63dff01d88acc2916c003489d">here</a>
so that you can try the function yourself. The difficult part is <code>str_extract("(?&lt;=CONTENT)(.*?)(?=WC)")</code>
which is were the words inside the “CONTENT” tag get extracted.</p>
<p>I then map this function to all the pages, and get a nice tibble with all the words:</p>
<pre class="r"><code>ad_words &lt;- map_dfr(ad_pages, get_words)</code></pre>
<pre class="r"><code>ad_words</code></pre>
<pre><code>## # A tibble: 1,114,662 x 2
##    tokens     page                            
##    &lt;chr&gt;      &lt;chr&gt;                           
##  1 afin       1877-01-05_01/text/1877-01-05_01
##  2 de         1877-01-05_01/text/1877-01-05_01
##  3 mettre     1877-01-05_01/text/1877-01-05_01
##  4 mes        1877-01-05_01/text/1877-01-05_01
##  5 honorables 1877-01-05_01/text/1877-01-05_01
##  6 clients    1877-01-05_01/text/1877-01-05_01
##  7 à          1877-01-05_01/text/1877-01-05_01
##  8 même       1877-01-05_01/text/1877-01-05_01
##  9 d          1877-01-05_01/text/1877-01-05_01
## 10 avantages  1877-01-05_01/text/1877-01-05_01
## # … with 1,114,652 more rows</code></pre>
<p>I then do some further cleaning, removing stop words (French and German, because there are some
ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed.
I also remove some German words from the few German ads that are in the paper, because they have
a very high tf-idf (I’ll explain below what that is).
I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their
clients with <em>honorable clientèle</em>, or used the word <em>vente</em>, and so on. This is what you see below
in the very long calls to <code>str_remove_all</code>. I also compute the <code>tf_idf</code> and I am grateful to
ThinkR blog post on that, which you can read <a href="https://thinkr.fr/text-mining-et-topic-modeling-avec-r/">here</a>.
It’s in French though, but the idea of the blog post is to present topic modeling with Wikipedia
articles. You can also read the section on tf-idf from the Text Mining with R ebook, <a href="https://www.tidytextmining.com/tfidf.html">here</a>.
tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf
of 0. So I use this to further remove very common words, by only keeping words with a tf-idf
greater than 0.01. This is why I manually remove garbage words and German words below, because they
are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words
I had to go back and forth between the tibble of cleaned words and my code, and manually add all
these exceptions. It took some time, but definitely made the results of the next steps better.<br />
I then use <code>cast_dtm</code> to cast the tibble into a DocumentTermMatrix object, which
is needed for the <code>LDA()</code> function that does the topic modeling:</p>
<pre class="r"><code>stopwords_fr &lt;- read_csv(&quot;https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt&quot;,
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre class="r"><code>stopwords_de &lt;- read_csv(&quot;https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt&quot;,
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre><code>## Warning: 1 parsing failure.
## row col  expected    actual                                                                                   file
## 157  -- 1 columns 2 columns &#39;https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt&#39;</code></pre>
<pre class="r"><code>ad_words2 &lt;- ad_words %&gt;% 
    filter(!is.na(tokens)) %&gt;% 
    mutate(tokens = str_remove_all(tokens, 
                                   &#39;[|\\|!|&quot;|#|$|%|&amp;|\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|’|\&#39;|‘|(|)|\\||~|=|]|°|&lt;|&gt;|«|»|\\d{1,100}|©|®|•|—|„|“|-|¦\\\\|”&#39;)) %&gt;%
    mutate(tokens = str_remove_all(tokens,
                                   &quot;j&#39;|j’|m’|m&#39;|n’|n&#39;|c’|c&#39;|qu’|qu&#39;|s’|s&#39;|t’|t&#39;|l’|l&#39;|d’|d&#39;|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferftücf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil&quot;)) %&gt;%
    anti_join(stopwords_de, by = c(&quot;tokens&quot; = &quot;X1&quot;)) %&gt;% 
    filter(!str_detect(tokens, &quot;§&quot;)) %&gt;% 
    mutate(tokens = ifelse(tokens == &quot;inédite&quot;, &quot;inédit&quot;, tokens)) %&gt;% 
    filter(tokens != &quot;&quot;) %&gt;% 
    anti_join(stopwords_fr, by = c(&quot;tokens&quot; = &quot;X1&quot;)) %&gt;% 
    count(page, tokens) %&gt;% 
    bind_tf_idf(tokens, page, n) %&gt;% 
    arrange(desc(tf_idf))

dtm_long &lt;- ad_words2 %&gt;% 
    filter(tf_idf &gt; 0.01) %&gt;% 
    cast_dtm(page, tokens, n)</code></pre>
<p>To read more details on this, I suggest you take a look at the following section of the
Text Mining with R ebook: <a href="https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>.</p>
<p>I choose to model 10 topics (<code>k = 10</code>), and set the <code>alpha</code> parameter to 5. This hyperparamater controls how
many topics are present in one document. Since my ads are all in one page (one document), I
increased it. Let’s fit the model, and plot the results:</p>
<pre class="r"><code>lda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))</code></pre>
<p>I plot the per-topic-per-word probabilities, the “beta” from the model and plot the 5 words that
contribute the most to each topic:</p>
<pre class="r"><code>result &lt;- tidy(lda_model_long, &quot;beta&quot;)

result %&gt;%
    group_by(topic) %&gt;%
    top_n(5, beta) %&gt;%
    ungroup() %&gt;%
    arrange(topic, -beta) %&gt;% 
    mutate(term = reorder(term, beta)) %&gt;%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = &quot;free&quot;) +
    coord_flip() +
    theme_blog()</code></pre>
<p><img src="../../blog/2019-01-04-newspapers_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>So some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made
out of leather. The word <code>semelle</code>, sole, also appears.
Then there’s a lot of topics that reference either music, bals, or instruments.
I guess these are ads for local music festivals, or similar events. There’s also an ad for what
seems to be bundles of sticks, topic 3: <code>chêne</code> is oak, <code>copeaux</code> is shavings and you know
what <code>fagots</code> is. The first word <code>stère</code> which I did not know is a unit of volume equal to one
cubic meter (see <a href="https://en.wikipedia.org/wiki/Stere">Wikipedia</a>). So they were likely selling
bundle of oak sticks by the cubic meter. For the other topics, I either
lack context or perhaps I just need to adjust <code>k</code>, the number of topics to model, and <code>alpha</code> to get better
results. In the meantime, topic 1 is about shoes (<code>chaussures</code>), theatre, fuel (<code>combustible</code>)
and farts (<code>pet</code>). Really wonder what they were selling in that shop.</p>
<p>In any case, this was quite an interesting project. I learned a lot about topic modeling
and historical newspapers of my country! I do not know if I will continue exploring it myself,
but I am really curious to see what others will do with it!</p>
<p>Hope you enjoyed! If you found this blog post useful, you might want to follow
me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates and
<a href="https://www.buymeacoffee.com/brodriguesco">buy me an espresso</a> or <a href="https://www.paypal.me/brodriguesco">paypal.me</a>.</p>
<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}</style>
<p><link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/brodriguesco"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me an Espresso"><span style="margin-left:5px">Buy me an Espresso</span></a></p>

      </div>
    </div>
  <div class="row">
    <div class="col-lg-9 col-md-9 col-sm-8 col-lg-offset-3 col-md-offset-3">
    <footer>
  <div class="row">
    <div class="col-lg-12">
      <p>Copyright 2020. <a> Bruno Rodrigues</a> </p>
      <p>Made with the HUGO theme <a href="https://github.com/adejoux/hugo-darkdoc-theme" rel="nofollow">darkdoc</a>.</p>
    </div>
  </div>
</footer>

</body>

    </div>
  </div>

</div>
</html>
<script>

  var api_url = '';

</script>
<script src="//code.jquery.com/jquery-2.2.3.min.js"></script>
<script src="../../js/application.js"></script>
<script type="text/javascript" src="../../js/jquery.fancybox.pack.js?v=2.1.5"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<script type="text/javascript">
  $(document).ready(function() {
    $(".fancybox").fancybox();
  });
</script>

