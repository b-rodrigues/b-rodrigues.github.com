<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"  lang="en-us">
    <title>From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack - Econometrics and Free Software</title>
    <meta name="generator" content="Hugo 0.25.1" />

    
    <meta name="description" content="A blog about econometrics, free software, and R">
    
    <link rel="canonical" href="../../blog/2018-11-01-nethack/">
    
    <meta name="author" content="Bruno Rodrigues">
    
    <meta name="generator" content="Hugo 0.25.1" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:url" content="/blog/2018-11-01-nethack/">
    <meta property="og:title" content="Econometrics and Free Software">
    <meta property="og:image" content="/map[url:logo.png width:50 height:50 alt:Logo]">
    <meta name="apple-mobile-web-app-title" content="Econometrics and Free Software">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="shortcut icon" type="image/x-icon" href="../../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../../images/favicon.ico">

    
    <link rel="stylesheet" href="../../css/bootstrap.min.css" media="screen">
    <link rel="stylesheet" href="../../css/custom.css">
    <link rel="stylesheet" href="../../css/pygments.css">
    
    <link rel="stylesheet" href="../../css/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
    <link rel="stylesheet" href="../../css/helpers/jquery.fancybox-buttons.css?v=1.0.5" type="text/css" media="screen" />
  </head>
  <body>


<div class="row">
<div class="navbar navbar-default navbar-fixed-top">
<div class="container">
  <div class="navbar-header">
    <a href="../../" class="navbar-brand">Econometrics and Free Software</a>
    <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
  </div>
  <div class="navbar-collapse collapse" id="navbar-main">

    <ul class="nav navbar-nav navbar-right">
      <li><a href="https://www.buymeacoffee.com/brodriguesco">Donate</a></li>
      <li><a href="https://www.youtube.com/c/BrunoRodrigues1988">Youtube</a></li>
      <li><a href="https://github.com/b-rodrigues/">Github</a></li>
      <li><a href="https://twitter.com/brodriguesco/">Twitter</a></li>
    </ul>
  </div>
</div>

</div>

<div class="container">

  <div>
    <div class="row">
      <div class="col-lg-3 col-md-3 col-sm-4">
        

<div class="list-group table-of-contents">

  
    <a class="list-group-item" href=#><b>About Me</b></a>
    
    <div class="list-group">
      
      <a class="list-group-item" href="../../about/about/">Who am I?</a>
      
      <a class="list-group-item" href="../../about/projects/">Projects</a>
      
      <a class="list-group-item" href="../../about/research/">Research</a>
      
      <a class="list-group-item" href="../../about/software/">Software</a>
      
    </div>
    
  
    <a class="list-group-item" href=#><b>Blog</b></a>
    
    <div class="list-group">
      
      <a class="list-group-item" href="../../blog/2018-11-25-tidy_cv/">A tutorial on tidy cross-validation with R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-03-nethack_analysis/">Analyzing NetHack data, part 1: What kills the players</a>
      
      <a class="list-group-item" href="../../blog/2018-11-10-nethack_analysis_part2/">Analyzing NetHack data, part 2: What players kill the most</a>
      
      <a class="list-group-item" href="../../blog/2019-02-04-newspapers_shiny_app_tutorial/">Building a shiny app to explore historical newspapers: a step-by-step guide</a>
      
      <a class="list-group-item" href="../../blog/2019-03-03-historical_vowpal/">Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</a>
      
      <a class="list-group-item" href="../../blog/2019-03-05-historical_vowpal_part2/">Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</a>
      
      <a class="list-group-item" href="../../blog/2019-06-20-tidy_eval_saga/">Curly-Curly, the successor of Bang-Bang</a>
      
      <a class="list-group-item" href="../../blog/2018-07-08-rob_stderr/">Dealing with heteroskedasticity; regression with robust standard errors using R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-14-luxairport/">Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport</a>
      
      <a class="list-group-item" href="../../blog/2018-10-05-ggplot2_purrr_officer/">Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer</a>
      
      <a class="list-group-item" href="../../blog/2019-04-28-diffindiff_part1/">Fast food, causality and R packages, part 1</a>
      
      <a class="list-group-item" href="../../blog/2019-05-04-diffindiff_part2/">Fast food, causality and R packages, part 2</a>
      
      <a class="list-group-item" href="../../blog/2019-05-18-xml2/">For posterity: install {xml2} on GNU/Linux distros</a>
      
      <a class="list-group-item" href="../../blog/2018-06-24-fun_ts/">Forecasting my weight with R</a>
      
      <a class="list-group-item" href="../../blog/2018-11-01-nethack/">From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack</a>
      
      <a class="list-group-item" href="../../blog/2019-03-31-tesseract/">Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</a>
      
      <a class="list-group-item" href="../../blog/2018-06-10-scraping_pdfs/">Getting data from pdfs using the pdftools package</a>
      
      <a class="list-group-item" href="../../blog/2018-10-21-lux_elections/">Getting the data from the Luxembourguish elections out of Excel</a>
      
      <a class="list-group-item" href="../../blog/2018-09-11-human_to_machine/">Going from a human readable Excel file to a machine-readable csv with {tidyxl}</a>
      
      <a class="list-group-item" href="../../blog/2019-04-07-historical_newspaper_scraping_tesseract/">Historical newspaper scraping with {tesseract} and R</a>
      
      <a class="list-group-item" href="../../blog/2018-09-15-time_use/">How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data</a>
      
      <a class="list-group-item" href="../../blog/2018-04-14-playing_with_furrr/">Imputing missing values in parallel using {furrr}</a>
      
      <a class="list-group-item" href="../../blog/2019-06-12-intermittent/">Intermittent demand, Croston and Die Hard</a>
      
      <a class="list-group-item" href="../../blog/2019-01-04-newspapers/">Looking into 19th century ads from a Luxembourguish newspaper with R</a>
      
      <a class="list-group-item" href="../../blog/2019-01-13-newspapers_mets_alto/">Making sense of the METS and ALTO XML standards</a>
      
      <a class="list-group-item" href="../../blog/2018-12-15-lubridate_africa/">Manipulate dates easily with {lubridate}</a>
      
      <a class="list-group-item" href="../../blog/2019-02-10-stringr_package/">Manipulating strings with the {stringr} package</a>
      
      <a class="list-group-item" href="../../blog/2018-10-27-lux_elections_analysis/">Maps with pie charts on top of each administrative division: an example with Luxembourg&#39;s elections data</a>
      
      <a class="list-group-item" href="../../blog/2018-07-01-tidy_ive/">Missing data imputation and instrumental variables regression: the tidy approach</a>
      
      <a class="list-group-item" href="../../blog/2019-08-17-modern_r/">Modern R with the tidyverse is available on Leanpub</a>
      
      <a class="list-group-item" href="../../blog/2018-12-24-modern_objects/">Objects types and some useful R functions for beginners</a>
      
      <a class="list-group-item" href="../../blog/2019-03-20-pivot/">Pivoting data frames just got easier thanks to `pivot_wide()` and `pivot_long()`</a>
      
      <a class="list-group-item" href="../../blog/2018-12-30-reticulate/">R or Python? Why not both? Using Anaconda Python within R with {reticulate}</a>
      
      <a class="list-group-item" href="../../blog/2018-11-15-tidy_gridsearch/">Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach</a>
      
      <a class="list-group-item" href="../../blog/2018-12-27-fun_gganimate/">Some fun with {gganimate}</a>
      
      <a class="list-group-item" href="../../blog/2019-10-05-parallel_maxlik/">Split-apply-combine for Maximum Likelihood Estimation of a linear model</a>
      
      <a class="list-group-item" href="../../blog/2019-07-19-statmatch/">Statistical matching, or when one single data source is not enough</a>
      
      <a class="list-group-item" href="../../blog/2018-11-21-lux_castle/">The best way to visit Luxembourguish castles is doing data science &#43; combinatorial optimization</a>
      
      <a class="list-group-item" href="../../blog/2019-05-19-spacemacs/">The never-ending editor war (?)</a>
      
      <a class="list-group-item" href="../../blog/2018-09-08-steam_linux/">The year of the GNU&#43;Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse</a>
      
      <a class="list-group-item" href="../../blog/2019-01-31-newspapers_shiny_app/">Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</a>
      
      <a class="list-group-item" href="../../blog/2018-11-16-rgenoud_arima/">Using a genetic algorithm for the hyperparameter optimization of a SARIMA model</a>
      
      <a class="list-group-item" href="../../blog/2019-06-04-cosine_sim/">Using cosine similarity to find matching documents: a tutorial using Seneca&#39;s letters to his friend Lucilius</a>
      
      <a class="list-group-item" href="../../blog/2019-08-14-lpm/">Using linear models with binary dependent variables, a simulation study</a>
      
      <a class="list-group-item" href="../../blog/2018-12-21-tidyverse_pi/">Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods</a>
      
      <a class="list-group-item" href="../../blog/2018-12-02-hyper-parameters/">What hyper-parameters are, and what to do with them; an illustration with ridge regression</a>
      
      <a class="list-group-item" href="../../blog/2019-09-03-disk_frame/">{disk.frame} is epic</a>
      
      <a class="list-group-item" href="../../blog/2018-04-15-announcing_pmice/">{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}</a>
      
      <a class="list-group-item" href="../../blog/2017-12-27-build_formulae/">Building formulae</a>
      
      <a class="list-group-item" href="../../blog/2017-11-14-peace_r/">Functional peace of mind</a>
      
      <a class="list-group-item" href="../../blog/2018-04-10-brotools_describe/">Get basic summary statistics for all the variables in a data frame</a>
      
      <a class="list-group-item" href="../../blog/2018-03-03-sparklyr_h2o_rsparkling/">Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash</a>
      
      <a class="list-group-item" href="../../blog/2018-02-16-importing_30gb_of_data/">Importing 30GB of data into R with sparklyr</a>
      
      <a class="list-group-item" href="../../blog/2017-03-27-introducing_brotools/">Introducing brotools</a>
      
      <a class="list-group-item" href="../../blog/2018-01-03-lists_all_the_way/">It&#39;s lists all the way down</a>
      
      <a class="list-group-item" href="../../blog/2018-01-05-lists_all_the_way2/">It&#39;s lists all the way down, part 2: We need to go deeper</a>
      
      <a class="list-group-item" href="../../blog/2018-03-12-keep_trying/">Keep trying that api call with purrr::possibly()</a>
      
      <a class="list-group-item" href="../../blog/2017-06-19-dplyr-0-70-tutorial/">Lesser known dplyr 0.7* tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-02-17-lesser_known_tricks/">Lesser known dplyr tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-03-24-lesser_known_purrr/">Lesser known purrr tricks</a>
      
      <a class="list-group-item" href="../../blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>
      
      <a class="list-group-item" href="../../blog/2018-01-19-mapping_functions_with_any_cols/">Mapping a list of functions to a list of datasets with a list of columns as arguments</a>
      
      <a class="list-group-item" href="../../blog/2018-02-11-census-random_forest/">Predicting job search by training a random forest on an unbalanced dataset</a>
      
      <a class="list-group-item" href="../../blog/2017-12-17-teaching_tidyverse/">Teaching the tidyverse to beginners</a>
      
      <a class="list-group-item" href="../../blog/2017-08-27-why_tidyeval/">Why I find tidyeval useful</a>
      
      <a class="list-group-item" href="../../blog/2017-07-27-spread_rename_at/">tidyr::spread() and dplyr::rename_at() in action</a>
      
      <a class="list-group-item" href="../../blog/2017-10-26-margins_r/">Easy peasy STATA-like marginal effects with R</a>
      
      <a class="list-group-item" href="../../blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/">Functional programming and unit testing for data munging with R available on Leanpub</a>
      
      <a class="list-group-item" href="../../blog/2017-02-17-how_to_use_jailbreakr/">How to use jailbreakr</a>
      
      <a class="list-group-item" href="../../blog/2017-01-07-my-free-book-has-a-cover/">My free book has a cover!</a>
      
      <a class="list-group-item" href="../../blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/">Work on lists of datasets instead of individual datasets by using functional programming</a>
      
      <a class="list-group-item" href="../../blog/2013-01-29-method-of-simulated-moments-with-r/">Method of Simulated Moments with R</a>
      
      <a class="list-group-item" href="../../blog/2012-12-11-new-website/">New website!</a>
      
      <a class="list-group-item" href="../../blog/2013-11-07-gmm-with-rmd/">Nonlinear Gmm with R - Example with a logistic regression</a>
      
      <a class="list-group-item" href="../../blog/2013-01-16-simulated-maximum-likelihood-with-r/">Simulated Maximum Likelihood with R</a>
      
      <a class="list-group-item" href="../../blog/2015-11-11-bootstrapping-did-with-r/">Bootstrapping standard errors for difference-in-differences estimation with R</a>
      
      <a class="list-group-item" href="../../blog/2016-06-21-careful-with-trycatch/">Careful with tryCatch</a>
      
      <a class="list-group-item" href="../../blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/">Data frame columns as arguments to dplyr functions</a>
      
      <a class="list-group-item" href="../../blog/2015-02-22-export-r-output-to-file/">Export R output to a file</a>
      
      <a class="list-group-item" href="../../blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/">I&#39;ve started writing a &#39;book&#39;: Functional programming and unit testing for data munging with R</a>
      
      <a class="list-group-item" href="../../blog/2015-01-12-introduction-to-programming-econometrics-with-r/">Introduction to programming econometrics with R</a>
      
      <a class="list-group-item" href="../../blog/2016-07-30-merge-a-list-of-datasets-together/">Merge a list of datasets together</a>
      
      <a class="list-group-item" href="../../blog/2014-04-23-r-s4-rootfinding/">Object Oriented Programming with R: An example with a Cournot duopoly</a>
      
      <a class="list-group-item" href="../../blog/2014-11-11-benchmarks-r-blas-atlas-rro/">R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?</a>
      
      <a class="list-group-item" href="../../blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/">Read a lot of datasets at once with R</a>
      
      <a class="list-group-item" href="../../blog/2016-03-31-unit-testing-with-r/">Unit testing with R</a>
      
      <a class="list-group-item" href="../../blog/2015-05-03-update-introduction-r-programming/">Update to Introduction to programming econometrics with R</a>
      
      <a class="list-group-item" href="../../blog/2013-12-31-r-cas/">Using R as a Computer Algebra System with Ryacas</a>
      
    </div>
    
  

</div>

      </div>
      <div class="col-lg-9 col-md-9 col-sm-8">
        <div>
           <h1 id="main">From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack</h1>
           <span class="article-date">November 1, 2018</span>
        </div>
        <blockquote class="twitter-tweet"><p lang="en" dir="ltr">If someone told me a decade ago (back before I&#39;d ever heard the term &quot;roguelike&quot;) what I&#39;d be doing today, I would have trouble believing this...<br><br>Yet here we are. <a href="https://t.co/N6Hh6A4tWl">pic.twitter.com/N6Hh6A4tWl</a></p>&mdash; Josh Ge (@GridSageGames) <a href="https://twitter.com/GridSageGames/status/1009664438683648001?ref_src=twsrc%5Etfw">June 21, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<div id="update-07-11-2018" class="section level2">
<h2>Update 07-11-2018</h2>
<p>The <code>{nethack}</code> package currently on Github contains a sample of 6000 NetHack games played
on the alt.org/nethack public server between April and November 2018. This data was kindly provided by <a href="https://twitter.com/paxed"><code>@paxed</code></a>.
The tutorial in this blog post is still useful if you want to learn more about scraping with R
and building a data package.</p>
</div>
<div id="abstract" class="section level2">
<h2>Abstract</h2>
<p>In this post, I am going to show you how you can scrape tables from a website, and then create a package
with the tidied data to share with the world. The data I am going to scrape comes from a NetHack
public server (<a href="https://alt.org/nethack/">link</a>). The data I discuss in this blog post is
available in the <code>{nethack}</code> package I created and I will walk you through the process of releasing
your package on CRAN. However, <code>{nethack}</code> is too large to be on CRAN (75 mb, while the maximum
allowed is 5mb), so you can install it to play around with the data from github:</p>
<pre class="r"><code>devtools::install_github(&quot;b-rodrigues/nethack&quot;)</code></pre>
<p>And to use it:</p>
<pre class="r"><code>library(nethack)
data(&quot;nethack&quot;)</code></pre>
<p>The data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I
will analyze the data in a future blog post. This post focuses on getting and then sharing the
data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.</p>
<p>I built the package by using the very useful <code>{devtools}</code> package.</p>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>NetHack is a game released in 1987 that is still being played and developed today.
NetHack is a roguelike game, meaning that is has procedurally generated dungeons and permadeath.
If you die, you have to start over, and because the dungeons are procedurally generated, this means
that you cannot learn the layout of the dungeons you explore or know when ennemies are going
to attack or even what ennemies are going to attack. Ennemies are not the only thing that you have
to be careful about; you can die from a lot of different events, as you will see in this post.
Objects that you find, such as a silver ring, might be helpful in a run, but be cursed in the next run.</p>
<p>The latest version of the game, 3.6.1, was released on April 27th 2018, and this is how it looks like:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/ff/Nethack.png" /><!-- --></p>
<p>The graphics are… bare-bones to say the least. The game runs inside a terminal emulator and is
available for any platform. The goal of NetHack is to explore a dungeon and go down every
level until you find the Amulet of Yendor. Once you find this Amulet, you have to go all the way
back upstairs, enter and fight your way through the Elemental Planes, enter the final Astral Plane,
and then finally offer the Amulet of Yendor to your god to finish the game. Needless to say,
NetHack is very difficult and players can go years without ever finishing the game.</p>
<p>When you start an new game, you have to create a character, which can have several attributes.
You have to choose a race (human, elf, orc, etc), a role (tourist, samurai, mage, etc) and an
alignment (neutral, law, chaos) and these choices impact your base stats.</p>
<p>If you can’t get past the ASCII graphics, you can play NetHack with tileset:</p>
<p><img src="https://vignette.wikia.nocookie.net/nethack/images/8/80/Vultures_eye.png/revision/latest?cb=20070313215112" /><!-- --></p>
<p>You can install NetHack on your computer or you can play online on a public server, such as
this <a href="https://alt.org/nethack/">one</a>. There are several advantages when playing on a pubic server;
the player does not have to install anyhing, and we data enthusiasts have access to a mine of
information! For example, you can view the following <a href="https://alt.org/nethack/gamesday.php?date=20181025">table</a>
which contains data on all the games played on October 25th 2018. These tables start in the year 2001,
and I am going to scrape the info from these tables, which will allow me to answer several questions.
For instance, what is the floor most players die on? What kills most players?
What role do players choose more often? I will explore this questions in a future blog post, but for
now I will focus on scraping the data and realeasing it as a package to CRAN.</p>
</div>
<div id="scraping-the-data" class="section level2">
<h2>Scraping the data</h2>
<p>To scrape the data I wrote a big function that does several things:</p>
<pre class="r"><code>library(&quot;tidyverse&quot;)
library(&quot;rvest&quot;)


scrape_one_day &lt;- function(link){

    convert_to_seconds &lt;- function(time_string){
        time_numeric &lt;- time_string %&gt;%
            str_split(&quot;:&quot;, simplify = TRUE) %&gt;%
            as.numeric
     
     time_in_seconds &lt;- time_numeric * c(3600, 60, 1)
     
     if(is.na(time_in_seconds)){
         time_in_seconds &lt;- 61
     } else {
         time_in_seconds &lt;- sum(time_in_seconds)
     }
     return(time_in_seconds)
    }

    Sys.sleep(1)

    date &lt;- str_extract(link, &quot;\\d{8}&quot;)

    read_lines_slow &lt;- function(...){
        Sys.sleep(1)
        read_lines(...)
    }
    
    page &lt;- read_html(link)

        # Get links
    dumplogs &lt;- page %&gt;% 
        html_nodes(xpath = &#39;//*[(@id = &quot;perday&quot;)]//td&#39;) %&gt;%
        html_children() %&gt;%
        html_attr(&quot;href&quot;) %&gt;%
        keep(str_detect(., &quot;dumplog&quot;))

    # Get table
    table &lt;- page %&gt;%
        html_node(xpath = &#39;//*[(@id = &quot;perday&quot;)]&#39;) %&gt;%
        html_table(fill = TRUE)

    if(is_empty(dumplogs)){
        print(&quot;dumplogs empty&quot;)
        dumplogs &lt;- rep(NA, nrow(table))
    } else {
        dumplogs &lt;- dumplogs
    }
    
    final &lt;- table %&gt;%
        janitor::clean_names() %&gt;%
        mutate(dumplog_links = dumplogs)

    print(paste0(&quot;cleaning data of date &quot;, date))
    
    clean_final &lt;- final %&gt;%
        select(-x) %&gt;%
        rename(role = x_2,
               race = x_3,
               gender = x_4,
               alignment = x_5) %&gt;%
        mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%
        filter(!(death %in% c(&quot;quit&quot;, &quot;escaped&quot;)), time_in_seconds &gt; 60) %&gt;%
        mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%
        mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))

    saveRDS(clean_final, paste0(&quot;datasets/data_&quot;, date, &quot;.rds&quot;))

}</code></pre>
<p>Let’s go through each part. The first part is a function that converts strings like “02:21:76” to
seconds:</p>
<pre class="r"><code>convert_to_seconds &lt;- function(time_string){
    time_numeric &lt;- time_string %&gt;%
        str_split(&quot;:&quot;, simplify = TRUE) %&gt;%
        as.numeric
 
time_in_seconds &lt;- time_numeric * c(3600, 60, 1)
 
if(is.na(time_in_seconds)){
  time_in_seconds &lt;- 61
  } else {
    time_in_seconds &lt;- sum(time_in_seconds)
    }
return(time_in_seconds)
}</code></pre>
<p>I will use this function on the column that gives the length of the run. However,
before March 2008 this column is always empty, this is why I have the <code>if()...else()</code> statement at
the end; if the time in seconds is <code>NA</code>, then I make it 61 seconds. I do this because I want to keep
runs longer than 60 seconds, something I use <code>filter()</code> for later. But when filtering, if the condition
returns <code>NA</code> (which happens when you do <code>NA &gt; 60</code>) then you get an error, and the function fails.</p>
<p>The website links I am going to scrape all have the date of the day the runs took place. I am going to
keep this date because I will need to name the datasets I am going to write to disk:</p>
<pre class="r"><code>date &lt;- str_extract(link, &quot;\\d{8}&quot;)</code></pre>
<p>Next, I define this function:</p>
<pre class="r"><code>read_lines_slow &lt;- function(...){
    Sys.sleep(1)
    read_lines(...)
}</code></pre>
<p>It is a wrapper around the <code>readr::read_lines()</code> with a call to <code>Sys.sleep(1)</code>. I will be scraping
a lot of pages, so letting one second pass between each page will not overload the servers so much.</p>
<p>I then read the link with <code>read_html()</code> and start by getting the links of the dumplogs:</p>
<pre class="r"><code>page &lt;- read_html(link)

# Get links
dumplogs &lt;- page %&gt;% 
    html_nodes(xpath = &#39;//*[(@id = &quot;perday&quot;)]//td&#39;) %&gt;%
    html_children() %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    keep(str_detect(., &quot;dumplog&quot;))</code></pre>
<p>You might be wondering what are dumplogs. Take a look at this screenshot:</p>
<pre class="r"><code>knitr::include_graphics(&quot;/img/dumplogs.png&quot;)</code></pre>
<p><img src="../../img/dumplogs.png" /><!-- --></p>
<p>When you click on those <code>d</code>‘s, you land on a page like this <a href="http://archive.is/wljb3">one</a> (I
archived it to be sure that this link will not die). These logs contain a lot of info that I want
to keep. To find the right ’xpath’ to scrape the links, "’//<em><span class="citation">[(@id = "perday")]</span>//td’", I used
the </em>SelectorGadget* extension for Chrome. First I chose the table:</p>
<pre class="r"><code>knitr::include_graphics(&quot;/img/selectorgadget1.png&quot;)</code></pre>
<p><img src="../../img/selectorgadget1.png" /><!-- --></p>
<p>and then the links I am interested in:</p>
<pre class="r"><code>knitr::include_graphics(&quot;/img/selectorgadget2.png&quot;)</code></pre>
<p><img src="../../img/selectorgadget2.png" /><!-- --></p>
<p>Putting them together, I get the right “xpath”. But just as with the time of the run, dumplogs are
only available after a certain date. So in case the <code>dumplogs</code> column is empty, I relpace it with <code>NA</code>.</p>
<pre class="r"><code>if(is_empty(dumplogs)){
    print(&quot;dumplogs empty&quot;)
    dumplogs &lt;- rep(NA, nrow(table))
} else {
    dumplogs &lt;- dumplogs
}</code></pre>
<p>The rest is quite simple:</p>
<pre class="r"><code># Get table
table &lt;- page %&gt;%
    html_node(xpath = &#39;//*[(@id = &quot;perday&quot;)]&#39;) %&gt;%
    html_table(fill = TRUE)
               
final &lt;- table %&gt;%
    janitor::clean_names() %&gt;%
    mutate(dumplog_links = dumplogs)
           
print(paste0(&quot;cleaning data of date &quot;, date))</code></pre>
<p>I scrape the table, and then join the dumplog links to the table inside a new column called “dumplog_links”.</p>
<p>Because what follows is a long process, I print a message to let me know the progress of the scraping.</p>
<p>Now the last part:</p>
<pre class="r"><code>clean_final &lt;- final %&gt;%
    select(-x) %&gt;%
    rename(role = x_2,
           race = x_3,
           gender = x_4,
           alignment = x_5) %&gt;%
    mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%
    filter(!(death %in% c(&quot;quit&quot;, &quot;escaped&quot;)), time_in_seconds &gt; 60) %&gt;%
    mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%
    mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))</code></pre>
<p>I first remove and remane columns. Then I convert the “time” column into seconds and also remove
runs that lasted less than 60 seconds or that ended either in “quit” (the player left the game)
or “escaped” (the player left the dungeon and the game ended immediately). There are a lot of runs
like that and they’re not interesting. Finally, and this is what takes long, I create a new
list-column where each element is the contents of the dumplog for that run. I wrap <code>read_lines_slow()</code>
around <code>purrr::possibly()</code> because dumplogs are missing for certains runs and when I try to read
them I get an 404 error back. Getting such an error stops the whole process, so with <code>purrr::possibly()</code>
I can specify that in that case I want <code>NA</code> back. Basically, a function wrapped inside <code>purrr::possibly()</code>
never fails! Finally, if a game lasts for 61 seconds, I convert it back to <code>NA</code> (remember this was
used to avoid having problems with the <code>filter()</code> function).</p>
<p>Finally, I export what I scraped to disk:</p>
<pre class="r"><code>saveRDS(clean_final, paste0(&quot;datasets/data_&quot;, date, &quot;.rds&quot;))</code></pre>
<p>This is where I use the date; to name the data. This is really important because scraping takes
a very long time, so if I don’t write the progress to disk as it goes, I might lose hours of work
if my internet goes down, or if computer freezes or whatever.</p>
<p>In the lines below I build the links that I am going to scrape. They’re all of the form:
<code>https://alt.org/nethack/gamesday.php?date=YYYYMMDD</code> so it’s quite easy to create a list of
dates to scrape, for example, for the year 2017:</p>
<pre class="r"><code>link &lt;- &quot;https://alt.org/nethack/gamesday.php?date=&quot;

dates &lt;- seq(as.Date(&quot;2017/01/01&quot;), as.Date(&quot;2017/12/31&quot;), by = &quot;day&quot;) %&gt;%
    str_remove_all(&quot;-&quot;)

links &lt;- paste0(link, dates)</code></pre>
<p>Now I can easily scrape the data. To make extra sure that I will not have problems during the
scraping process, for example if on a given day no games were played (and thus there is no table
to scrape, which would result in an error) , I use the same trick as above by using <code>purrr::possibly()</code>:</p>
<pre class="r"><code>map(links, ~possibly(scrape_one_day, otherwise = NULL)(.))</code></pre>
<p>The scraping process took a very long time. I scraped all the data by letting my computer run for
three days!</p>
<p>After this long process, I import all the <code>.rds</code> files into R:</p>
<pre class="r"><code>path_to_data &lt;- Sys.glob(&quot;datasets/*.rds&quot;)
nethack_data &lt;- map(path_to_data, readRDS)</code></pre>
<p>and take a look at one of them:</p>
<pre class="r"><code>nethack_data[[5812]] %&gt;% 
  View()</code></pre>
<p>Let’s convert the “score” column to integer. For this, I will need to convert strings that look
like “12,232” to integers. I’ll write a short function to do this:</p>
<pre class="r"><code>to_numeric &lt;- function(string){
  str_remove_all(string, &quot;,&quot;) %&gt;%
    as.numeric
}</code></pre>
<pre class="r"><code>nethack_data &lt;- nethack_data %&gt;%
  map(~mutate(., score = to_numeric(score)))</code></pre>
<p>Let’s merge the data into a single data frame:</p>
<pre class="r"><code>nethack_data &lt;- bind_rows(nethack_data)</code></pre>
<p>Now that I have a nice data frame, I will remove some columns and start the process of making a
packages. I remove the columns that I created and that are now useless (such as the <code>dumplog_links</code>
column).</p>
<pre class="r"><code>nethack_data &lt;- nethack_data %&gt;%
  select(rank, score, name, time, turns, lev_max, hp_max, role, race, gender, alignment, death,
         date, dumplog)</code></pre>
<p>Export this to <code>.rds</code> format, as it will be needed later:</p>
<pre class="r"><code>saveRDS(nethack_data, &quot;nethack_data.rds&quot;)</code></pre>
</div>
<div id="making-a-package-to-share-your-data-with-the-world" class="section level2">
<h2>Making a package to share your data with the world</h2>
<p>As stated in the beginning of this post, I will walk you through the process of creating and
releasing your package on CRAN. However, the data I scraped was too large to be made available
as a CRAN package. But you can still get the data from Github (link is in the abstract at the
beginning of the post).</p>
<p>Making a data package is a great way to learn how to make packages, because it is relatively easy
to do (for example, you do not need to write unit tests). First, let’s start a new project
in RStudio:</p>
<p><img src="../../img/r_package1.png" /><!-- --></p>
<p>Then select “R package”:</p>
<p><img src="../../img/r_package2.png" /><!-- --></p>
<p>Then name your package, create a git repository and then click on “Create Project”:</p>
<p><img src="../../img/r_package3.png" /><!-- --></p>
<p>RStudio wil open the <code>hello.R</code> script which you can now modify. You got to learn from the best, so
I suggest that you modify <code>hello.R</code> by taking inspiration from the <code>babynames</code> package made by Hadley
Wickham which you can find <a href="https://github.com/hadley/babynames/blob/master/R/data.R">here</a>.
You do not need the first two lines, and can focus on lines 4 to 13. Then, rename the script to
<code>data.R</code>. This is how <code>{nethack}'s</code> looks like:</p>
<pre class="r"><code>#&#39; NetHack runs data.
#&#39;
#&#39; Data on NetHack runs scraped from https://alt.org/nethack/gamesday.php
#&#39;
#&#39; @format A data frame with 14 variables: \code{rank}, \code{score},
#&#39;   \code{name}, \code{time}, \code{turns}, \code{lev_max}, \code{hp_max}, \code{role}, \code{race},
#&#39;   \code{gender}, \code{alignment}, \code{death}, \code{date} and \code{dumplog}
#&#39; \describe{
#&#39; \item{rank}{The rank of the player on that day}
#&#39; \item{score}{The score the player achieved on that run}
#&#39; \item{name}{The name of the player}
#&#39; \item{time}{The time the player took to finish the game}
#&#39; \item{turns}{The number of turns the player played before finishing the game}
#&#39; \item{lev_max}{First digit: the level the player died on; second digit: the deepest explored level}
#&#39; \item{hp_max}{The maximum character health points the player achieved}
#&#39; \item{role}{The role the player chose to play as}
#&#39; \item{race}{The race the player chose to play as}
#&#39; \item{gender}{The gender the playr chose to play as}
#&#39; \item{alignement}{The alignement the playr chose to play as}
#&#39; \item{death}{The reason of death of the character}
#&#39; \item{date}{The date the game took place}
#&#39; \item{dumplog}{The log of the end game; this is a list column}
#&#39; }
&quot;nethack&quot;</code></pre>
<p>The comments are special, the “#” is followed by a <code>'</code>; these are special comments that will be
parsed by <code>roxygen2::roxygenise()</code> and converted to documentation files.</p>
<p>Next is the <code>DESCRIPTION</code> file. Here is how <code>{nethack}</code>’s looks like:</p>
<pre><code>Package: nethack
Type: Package
Title: Data from the Video Game NetHack
Version: 0.1.0
Authors@R: person(&quot;Bruno André&quot;, &quot;Rodrigues Coelho&quot;, email = &quot;bruno@brodrigues.co&quot;,
                  role = c(&quot;aut&quot;, &quot;cre&quot;))
Description: Data from NetHack runs played between 2001 to 2018 on 
    &lt;https://alt.org/nethack/&gt;, a NetHack public server.
Depends: R (&gt;= 2.10)
License: CC BY 4.0
Encoding: UTF-8
LazyData: true
RoxygenNote: 6.1.0</code></pre>
<p>Adapt yours accordingly. I chose the license <code>CC BY 4.0</code> because this was the licence under which
the original data was published. It is also a good idea to add a <em>Vignette</em>:</p>
<pre class="r"><code>devtools::use_vignette(&quot;the_nethack_package&quot;)</code></pre>
<p>Vignettes are very useful documentation with more details and examples.</p>
<p>It is also good practice to add the script that was used to scrape the data. Such scripts go into
<code>data-raw/</code>. Create this folder with:</p>
<pre class="r"><code>devtools::use_data_raw()</code></pre>
<p>This creates the <code>data-raw/</code> folder where I save the script that scrapes the data. Now is time to
put the data in the package. Start by importing the data:</p>
<pre class="r"><code>nethack &lt;- readRDS(&quot;nethack_data.rds&quot;)</code></pre>
<p>To add the data to your package, you can use the following command:</p>
<pre class="r"><code>devtools::use_data(nethack, compress = &quot;xz&quot;)</code></pre>
<p>This will create the <code>data/</code> folder and put the data in there in the <code>.rda</code> format. I use the “compress”
option to make the data smaller. You can now create the documentation by running:</p>
<pre class="r"><code>roxygen2::roxygenise()</code></pre>
<p>Pay attention to the log messages: you might need to remove files (for example the documentation
<code>hello.R</code>, under the folder <code>man/</code>).</p>
<p>Now you can finaly run <code>R CMD Check</code> by clicking the <code>Check</code> button on the “Build” pane:</p>
<p><img src="../../img/r_package_check.png" /><!-- --></p>
<p>This will extensively check the package for <code>ERRORS</code>, <code>WARNINGS</code> and <code>NOTES</code>. You need to make sure
that the check passes without any <code>ERRORS</code> or <code>WARNINGS</code> and try as much as possible to remove all
<code>NOTES</code> too. If you cannot remove a <code>NOTE</code>, for example in my case the following:</p>
<pre><code>checking installed package size ... NOTE
  installed size is 169.7Mb
  sub-directories of 1Mb or more:
    data  169.6Mb
R CMD check results
0 errors | 0 warnings  | 1 note </code></pre>
<p>You should document it in a new file called <code>cran-comments.md</code>:</p>
<pre><code>## Test environments
* local openSUSE Tumbleweed install, R 3.5.1
* win-builder (devel and release)

## R CMD check results
There were no ERRORs or WARNINGs.

There was 1 NOTE:

    *   installed size is 169.7Mb
sub-directories of 1Mb or more:
    data  169.6Mb

The dataset contains 17 years of NetHack games played, hence the size. This package will not be updated often (max once a year).</code></pre>
<p>Once you have eliminated all errors and warnings, you are almost ready to go.</p>
<p>You need now to test the package on different platforms. This depends a bit on the system you run,
for me, because I run openSUSE (a GNU+Linux distribution) I have to test on Windows. This can be done
with:</p>
<pre class="r"><code> devtools::build_win(version = &quot;R-release&quot;)</code></pre>
<p>and:</p>
<pre class="r"><code> devtools::build_win(version = &quot;R-devel&quot;)</code></pre>
<p>Explain that you have tested your package on several platforms in the <code>cran-comments.md</code> file.</p>
<p>Finally you can add a <code>README.md</code> and a <code>NEWS.md</code> file and start the process of publishing the
package on CRAN:</p>
<pre class="r"><code>devtools:release()</code></pre>
<p>If you want many more details than what you can find in this blog post, I urge you to read
“R Packages” by Hadley Wickham, which you can read for free <a href="http://r-pkgs.had.co.nz/">here</a>.</p>
<p>Hope you enjoyed! If you found this blog post useful, you might want to follow
me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates or
<a href="https://www.buymeacoffee.com/brodriguesco">buy me an espresso</a>.</p>
<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}</style>
<p><link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/brodriguesco"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me an Espresso"><span style="margin-left:5px">Buy me an Espresso</span></a></p>
</div>

      </div>
    </div>
  <div class="row">
    <div class="col-lg-9 col-md-9 col-sm-8 col-lg-offset-3 col-md-offset-3">
    <footer>
  <div class="row">
    <div class="col-lg-12">
      <p>Copyright 2020. <a> Bruno Rodrigues</a> </p>
      <p>Made with the HUGO theme <a href="https://github.com/adejoux/hugo-darkdoc-theme" rel="nofollow">darkdoc</a>.</p>
    </div>
  </div>
</footer>

</body>

    </div>
  </div>

</div>
</html>
<script>

  var api_url = '';

</script>
<script src="//code.jquery.com/jquery-2.2.3.min.js"></script>
<script src="../../js/application.js"></script>
<script type="text/javascript" src="../../js/jquery.fancybox.pack.js?v=2.1.5"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../../js/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<script type="text/javascript">
  $(document).ready(function() {
    $(".fancybox").fancybox();
  });
</script>

